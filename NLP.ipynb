{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMrv/qYQ6SUZ8ccaqI1Ctbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanikakaushik12/ML-PROJECT/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "L8YIwzTEpFxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eef326b-14e9-456d-de6a-b633ad26a0a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/\n"
      ],
      "metadata": {
        "id": "7Fm8au30pKoW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!  chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "UdEQP7u6pzG2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g93wCcdp5tg",
        "outputId": "dc442b0a-c687-484e-d47a-42f7f151df53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading jigsaw-multilingual-toxic-comment-classification.zip to /content\n",
            "100% 1.08G/1.08G [00:15<00:00, 99.0MB/s]\n",
            "100% 1.08G/1.08G [00:15<00:00, 74.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip jigsaw-multilingual-toxic-comment-classification.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gao6JntCpxoW",
        "outputId": "e383b3e1-4f0f-4f4b-ef20-ac24ac669b43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  jigsaw-multilingual-toxic-comment-classification.zip\n",
            "  inflating: jigsaw-toxic-comment-train-processed-seqlen128.csv  \n",
            "  inflating: jigsaw-toxic-comment-train.csv  \n",
            "  inflating: jigsaw-unintended-bias-train-processed-seqlen128.csv  \n",
            "  inflating: jigsaw-unintended-bias-train.csv  \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test-processed-seqlen128.csv  \n",
            "  inflating: test.csv                \n",
            "  inflating: test_labels.csv         \n",
            "  inflating: validation-processed-seqlen128.csv  \n",
            "  inflating: validation.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import  tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "from plotly import graph_objects as go\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers  import LSTM, GRU, SimpleRNN\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import nltk\n",
        "import re"
      ],
      "metadata": {
        "id": "GLuEn1e64p8n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eqBf2v5SukVs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  print('Running on TPU', tpu.master())\n",
        "except ValueError:\n",
        "  tpu = None\n",
        "\n",
        "if tpu:\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.iniitialized_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS:\", strategy.num_replicas_in_sync)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isGaV-xxHFEH",
        "outputId": "2b8eec92-adb6-4819-82a5-b33eba9ce2cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICAS: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/jigsaw-toxic-comment-train.csv')\n",
        "validation = pd.read_csv('/content/validation.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n"
      ],
      "metadata": {
        "id": "l1_ZVWO9QXpw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)\n"
      ],
      "metadata": {
        "id": "AsLOnmrTTtJ3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.loc[:12000,:]\n",
        "train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFEiKvzrWwxy",
        "outputId": "a20aac3f-ebf4-498b-cde2-3f866d5b8a2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12001, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['comment_text'].apply(lambda x:len(str(x).split())).max()"
      ],
      "metadata": {
        "id": "Ta_RsHXdW5PB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e37252-bd96-494b-d066-e742e0279d78"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1403"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_auc(predictions, target):\n",
        "\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  return roc_auc"
      ],
      "metadata": {
        "id": "3RkpuilzYg5s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain,xvalid,ytrain,yvalid= train_test_split(train.comment_text.values, train.toxic.values,stratify=train.toxic.values,random_state=42,test_size=0.2,shuffle=True)"
      ],
      "metadata": {
        "id": "BAj1hfe0YuXu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = text.Tokenizer(num_words=None)\n",
        "max_len = 1500\n",
        "token.fit_on_texts(list(xtrain)+list(xvalid))\n",
        "xtrain_seq = token.texts_to_sequences(xtrain)\n",
        "xvalid_seq = token.texts_to_sequences(xvalid)\n",
        "xtrain_pad = sequence.pad_sequences(xtrain_seq,maxlen=max_len)\n",
        "xvalid_pad = sequence.pad_sequences(xvalid_seq,maxlen=max_len)\n",
        "word_index = token.word_index\n"
      ],
      "metadata": {
        "id": "wRk1ZBlyZUq_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "  model= Sequential()\n",
        "  model.add(Embedding(len(word_index)+1,300,input_length=max_len))\n",
        "  model.add(SimpleRNN(100))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx4SPiGFZ5dF",
        "outputId": "24002b4a-e605-4ec2-970f-924cfd2b1e83"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1500, 300)         13049100  \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 100)               40100     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13089301 (49.93 MB)\n",
            "Trainable params: 13089301 (49.93 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "CPU times: user 470 ms, sys: 132 ms, total: 602 ms\n",
            "Wall time: 752 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbf9PsZ_bERX",
        "outputId": "d97375d1-6027-4fa5-9d68-45b97a32b7c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "150/150 [==============================] - 562s 4s/step - loss: 0.3123 - accuracy: 0.8951\n",
            "Epoch 2/5\n",
            "150/150 [==============================] - 549s 4s/step - loss: 0.1054 - accuracy: 0.9609\n",
            "Epoch 3/5\n",
            "150/150 [==============================] - 547s 4s/step - loss: 0.0117 - accuracy: 0.9974\n",
            "Epoch 4/5\n",
            "150/150 [==============================] - 541s 4s/step - loss: 0.0018 - accuracy: 0.9999\n",
            "Epoch 5/5\n",
            "150/150 [==============================] - 545s 4s/step - loss: 6.3921e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d9abc73a770>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ91dwxAg9-C",
        "outputId": "b8aafa08-fa96-4578-d8ca-162d50f1e070"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76/76 [==============================] - 29s 376ms/step\n",
            "Auc: 0.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_model =[]\n",
        "scores_model.append({'Model': 'SimpleRNN','AUC_Score':roc_auc(scores,yvalid)})"
      ],
      "metadata": {
        "id": "EOW2vqnokqQO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain_seq[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mNcL1UKlGgZ",
        "outputId": "ffe83821-fa50-463c-f8b2-6dcc015b14f5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[664,\n",
              "  65,\n",
              "  7,\n",
              "  19,\n",
              "  2262,\n",
              "  14102,\n",
              "  5,\n",
              "  2262,\n",
              "  20439,\n",
              "  6071,\n",
              "  4,\n",
              "  71,\n",
              "  32,\n",
              "  20440,\n",
              "  6620,\n",
              "  39,\n",
              "  6,\n",
              "  664,\n",
              "  65,\n",
              "  11,\n",
              "  8,\n",
              "  20441,\n",
              "  1502,\n",
              "  38,\n",
              "  6072]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Correct file path\n",
        "glove_file_path = r'C:\\glove_file.csv'\n",
        "\n",
        "# Initialize the embeddings index dictionary\n",
        "embeddings_index = {}\n",
        "\n",
        "# Open the GloVe file in read mode with UTF-8 encoding\n",
        "try:\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Reading GloVe file\"):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    print('Found %s word vectors.' % len(embeddings_index))\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the path.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV2SZCpq5niP",
        "outputId": "9a7077ff-232c-4d79-fd60-77281003c1d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found. Please check the path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((len(word_index)+1,300))\n",
        "for word,i in tqdm(word_index.items()):\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPbiuNe-nk2k",
        "outputId": "849de4f3-248b-46ae-8bd0-69911bc9aefb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 43496/43496 [00:00<00:00, 1033002.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(len(word_index)+1,300,weights=[embedding_matrix],input_length=max_len,trainable=False))\n",
        "  model.add(LSTM(100,dropout=0.3,recurrent_dropout=0.3))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxmXnaKHoB-q",
        "outputId": "2960881b-bcd9-4df6-c65a-59e5015c8a0d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 1500, 300)         13049100  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13209601 (50.39 MB)\n",
            "Trainable params: 160501 (626.96 KB)\n",
            "Non-trainable params: 13049100 (49.78 MB)\n",
            "_________________________________________________________________\n",
            "CPU times: user 413 ms, sys: 270 ms, total: 683 ms\n",
            "Wall time: 603 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xtrain_pad,ytrain,epochs=5,batch_size=64*strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTJ8T514ocnD",
        "outputId": "327a595b-b6b5-4481-96dc-0f480a1139da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "150/150 [==============================] - 1709s 11s/step - loss: 0.3377 - accuracy: 0.9053\n",
            "Epoch 2/5\n",
            "150/150 [==============================] - 1688s 11s/step - loss: 0.3140 - accuracy: 0.9053\n",
            "Epoch 3/5\n",
            "150/150 [==============================] - 1797s 12s/step - loss: 0.3148 - accuracy: 0.9053\n",
            "Epoch 4/5\n",
            " 25/150 [====>.........................] - ETA: 26:11 - loss: 0.3261 - accuracy: 0.9000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"AUC: %.2f%%\" % (roc_auc(scores,yvalid)))"
      ],
      "metadata": {
        "id": "uZXYhCYeojgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_model.append({'Model': 'LSTM','AUC_Score':roc_auc(scores,yvalid)})"
      ],
      "metadata": {
        "id": "WAx3f2ICo12r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(len(word_index)+1,300,weights=[embedding_matrix],input_length=max_len,trainable=False))\n",
        "  model.add(SpatialDropout1D(0.3))\n",
        "  model.add(GRU(300))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "PDgzHUv-o8Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "id": "YVbKzQ8OpVOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"AUC: %.2f%%\" % (roc_auc(scores,yvalid)))"
      ],
      "metadata": {
        "id": "EEOVD2PcptPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_model.append({'Model':'GRU','AUC_Score':roc_auc(scores,yvalid)})"
      ],
      "metadata": {
        "id": "BnxaVOmTp1cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_model"
      ],
      "metadata": {
        "id": "38LymXhYqJft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "with Strategy.scope():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(len(word_index)+1,300,weights=[embedding_matrix],input_length=max_len,trainable=False))\n",
        "  model.add(Bidirectioanl(LSTM(300, dropout =0.3 ,recurrent_dropout=0.3)))\n",
        "  model.add(Dense(1,actiivation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LKiNV71hqNSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xtrain_pad , ytrain,epoch=5,batch_size=64*strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "id": "SNEgRn92rC6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"AUC: %.2f%%\" % (roc_auc(scores,yvalid)))"
      ],
      "metadata": {
        "id": "aj2prnglrYA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_model.append({'Model':'Bidirectional LSTM','AUC_Score':roc_auc(scores,yvalid)})"
      ],
      "metadata": {
        "id": "vr7w33ByrgBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.DataFrame(scores_model).sort_values(by='AUC_Score',ascending=False)\n",
        "results.style.background_gradient(cmap='Purples')"
      ],
      "metadata": {
        "id": "34BGodpIqNdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure(go.Funnelarea(text =results.Model,values =result.AUC_Score,title={\"position\":\"top center\",\"text\":\"Funnel-Chart of sentiment Distribution\"}))\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "UbANvqQszxpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-0GxxndD1LdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import transformers\n",
        "from transformers import BertWordPieceTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors"
      ],
      "metadata": {
        "id": "xKyowW05zxwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train1 = pd.read_csv('/content/jigsaw-toxic-comment-train.csv')\n",
        "valid = pd.read_csv('/content/validation.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "sub = pd.read_csv('/content/sample_submission.csv')"
      ],
      "metadata": {
        "id": "lBUmvWsIzx4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
        "    tokenizer.enable_truncation(max_length=maxlen)\n",
        "    tokenizer.enable_padding(max_length=maxlen)\n",
        "    all_ids = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
        "        text_chunk = texts[i:i+chunk_size].tolist()\n",
        "        encs = tokenizer.encode_batch(text_chunk)\n",
        "        all_ids.extend([enc.ids for enc in encs])\n",
        "    return np.array(all_ids)"
      ],
      "metadata": {
        "id": "zsQNMFLm3qZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Configuration\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "MAX_LEN = 192\n"
      ],
      "metadata": {
        "id": "BWM8jSeN3qwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "tokenizer.save_pretrained('.')\n",
        "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
        "fast_tokenizer"
      ],
      "metadata": {
        "id": "4V6DXc3i3rCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_x_train = fast_enode(train1.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "x_valid = fast_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "x_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "y_train = train1.toxic.values\n",
        "y_valid = valid.toxic.values"
      ],
      "metadata": {
        "id": "jUtkqtPS5stU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_valid, y_valid))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(x_test)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "V8wDs_ZD6U_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(transformer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "\n",
        "    model = Model(inputs=input_word_ids ,outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Q-5OYXGr6g12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    transformer_layer = (\n",
        "        transformers.TFDistilBertModel\n",
        "        .from_pretrained('distilbert-base-multilingual-cased')\n",
        "    )\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "W9M7fOVH7DQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "1SxNyTuX7Dnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "train_history_2 = model.fit(\n",
        "    valid_dataset.repeat(),\n",
        "    steps_per_epoch=n_steps,\n",
        "    epochs=EPOCHS*2\n",
        ")"
      ],
      "metadata": {
        "id": "Vvwu6Guk7Zz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
        "sub.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "s3Dz0c_97k2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}